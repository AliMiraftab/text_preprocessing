{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbce7b4f",
   "metadata": {},
   "source": [
    "# Spell Correction\n",
    "\n",
    "Spelling correction is a crucial preprocessing step in NLP tasks to ensure that the text data is clean and standardized. Here are some common methods and tools you can use for spelling correction:\n",
    "\n",
    "- TextBlob\n",
    "- PySpellChecker\n",
    "- SymSpell\n",
    "- Hunspell\n",
    "- BERT for Contextual Spell Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531fa92",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "TextBlob is a simple library for processing textual data. It provides a straightforward way to perform spell correction. \n",
    "To get the most out of TextBlob, download its corpora: `python -m textblob.download_corpora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a spelling mistake\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(text):\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    return corrected_text\n",
    "\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4929f0",
   "metadata": {},
   "source": [
    "## PySpellChecker\n",
    "\n",
    "PySpellChecker is a pure Python spell checking library that provides fast and easy spell checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b40f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a spelling mistake\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_text = \" \".join([spell.correction(word) for word in words])\n",
    "    return corrected_text\n",
    "\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89713bf",
   "metadata": {},
   "source": [
    "## SymSpell\n",
    "\n",
    "SymSpell is a very efficient spell checker and corrector for processing large amounts of text data. It's designed for high-performance; much higher speed and lower memory consumption.\n",
    "\n",
    "Note: Copy `frequency_dictionary_en_82_765.txt` (found in the inner symspellpy directory) to your project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d560ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have a spelling mistake\n"
     ]
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Initialize the spell checker\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Load the dictionary\n",
    "dictionary_path = \"../data/dict/frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    if suggestions:\n",
    "        return suggestions[0].term\n",
    "    return text\n",
    "\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e56699",
   "metadata": {},
   "source": [
    "## Hunspell\n",
    "\n",
    "[Hunspell](https://hunspell.github.io/) is a spell checker and morphological analyzer designed for languages with rich morphology and complex word compounding and character encoding.\n",
    "\n",
    "To install it on Mac, follow this [link](https://pankdm.github.io/hunspell.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c51a109",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hunspell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhunspell\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the spell checker\u001b[39;00m\n\u001b[1;32m      4\u001b[0m h \u001b[38;5;241m=\u001b[39m hunspell\u001b[38;5;241m.\u001b[39mHunSpell(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/share/hunspell/en_US.dic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/share/hunspell/en_US.aff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hunspell'"
     ]
    }
   ],
   "source": [
    "import hunspell\n",
    "\n",
    "# Initialize the spell checker\n",
    "h = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_text = \" \".join([h.suggest(word)[0] if h.suggest(word) else word for word in words])\n",
    "    return corrected_text\n",
    "\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60684958",
   "metadata": {},
   "source": [
    "## BERT for Contextual Spell Checking\n",
    "\n",
    "For more advanced and context-aware spell checking, you can use transformer models like BERT. This approach involves using a pre-trained language model to identify and correct misspelled words based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c00350b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8c953e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.09278012067079544, 'token': 1012, 'token_str': '.', 'sequence': '[CLS] i h. vv [MASK] speling mist [MASK] kke [SEP]'}, {'score': 0.09081113338470459, 'token': 2102, 'token_str': '##t', 'sequence': '[CLS] i ht vv [MASK] speling mist [MASK] kke [SEP]'}, {'score': 0.0415608249604702, 'token': 1010, 'token_str': ',', 'sequence': '[CLS] i h, vv [MASK] speling mist [MASK] kke [SEP]'}, {'score': 0.02465035952627659, 'token': 1041, 'token_str': 'e', 'sequence': '[CLS] i h e vv [MASK] speling mist [MASK] kke [SEP]'}, {'score': 0.02280365116894245, 'token': 2140, 'token_str': '##l', 'sequence': '[CLS] i hl vv [MASK] speling mist [MASK] kke [SEP]'}]\n",
      "i am am good .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def correct_spelling(text):\n",
    "    fill_mask = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    words = text.split()\n",
    "    corrected_text = []\n",
    "    for word in words:\n",
    "        masked_text = text.replace(word, '[MASK]')\n",
    "        predictions = fill_mask(masked_text)\n",
    "        \n",
    "        try: \n",
    "            corrected_word = predictions[0]['token_str'] if predictions else word\n",
    "        except:\n",
    "            print(predictions[0])\n",
    "        corrected_text.append(corrected_word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df841c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a good .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_text = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        masked_sentence = words.copy()\n",
    "        masked_sentence[i] = tokenizer.mask_token\n",
    "        masked_sentence = \" \".join(masked_sentence)\n",
    "        \n",
    "        # Encode the masked sentence\n",
    "        input_ids = tokenizer.encode(masked_sentence, return_tensors='pt')\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs.logits\n",
    "        \n",
    "        # Get the predicted token\n",
    "        mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "        predicted_token_id = predictions[0, mask_token_index, :].argmax(dim=-1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "        \n",
    "        # If the predicted token is different from the original word, replace it\n",
    "        if predicted_token != word:\n",
    "            corrected_text.append(predicted_token)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    \n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "# Example usage\n",
    "text = \"I havv a speling mistakke\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)  # Output: I have a spelling mistake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9cab69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08409816026687622,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new',\n",
       "  'sequence': 'i have a new mistakke.'},\n",
       " {'score': 0.06373114138841629,\n",
       "  'token': 2210,\n",
       "  'token_str': 'little',\n",
       "  'sequence': 'i have a little mistakke.'},\n",
       " {'score': 0.05808905139565468,\n",
       "  'token': 2204,\n",
       "  'token_str': 'good',\n",
       "  'sequence': 'i have a good mistakke.'},\n",
       " {'score': 0.020978424698114395,\n",
       "  'token': 2919,\n",
       "  'token_str': 'bad',\n",
       "  'sequence': 'i have a bad mistakke.'},\n",
       " {'score': 0.019894922152161598,\n",
       "  'token': 2307,\n",
       "  'token_str': 'great',\n",
       "  'sequence': 'i have a great mistakke.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")\n",
    "# I havv a speling mistakke\n",
    "unmasker(\"I have a [MASK] mistakke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d858f",
   "metadata": {},
   "source": [
    "## ContextualSpellCheck\n",
    "\n",
    "[contextualSpellCheck](https://github.com/R1j1t/contextualSpellCheck) This package is based on BERT and currently focuses on Out of Vocabulary (OOV) word or non-word error (NWE) correction using BERT model. The idea of using BERT was to use the context when correcting OOV. \n",
    "\n",
    "This package is under improvement to extend the functionality to identify RWE, optimising the package, and improving the documentation. \n",
    "\n",
    "To install this package you need: \n",
    "- `pip install git+https://github.com/roy-ht/editdistance.git@v0.6.2`\n",
    "- `pip install contextualSpellCheck`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "774bb355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "I have av attack.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "doc = nlp('I havv a speling mistakke.')\n",
    "\n",
    "print(doc._.performed_spellCheck) #Should be True\n",
    "print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb580368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: I have av attack.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "# Load a spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add the contextual spell checker to the spaCy pipeline\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "# Example text with a spelling error\n",
    "text = \"I havv a speling mistakke.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Check for spelling errors\n",
    "if doc._.performed_spellCheck:\n",
    "    corrected_text = doc._.outcome_spellCheck\n",
    "    print(\"Corrected Text:\", corrected_text)\n",
    "else:\n",
    "    print(\"No spelling errors found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd7d60",
   "metadata": {},
   "source": [
    "## Some other packages\n",
    "\n",
    "https://github.com/neuspell/neuspell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2d183",
   "metadata": {},
   "source": [
    "## Grammar Error Correction (GEC) with BERT\n",
    "\n",
    "This library: https://github.com/sunilchomal/GECwBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ffd07d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
