{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d2d07e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important.\n",
    "Objective of this kernel is to understand the various text preprocessing steps with code examples.\n",
    "Some of the common text preprocessing / cleaning steps are:\n",
    "- Lower Casing\n",
    "- Spelling Correction\n",
    "- Remove: \n",
    "    - Punctuations\n",
    "    - Special Chracters\n",
    "    - Expand Contractions (optinal deoend on the NLP model)\n",
    "    - Stop words\n",
    "    - Frequent Words\n",
    "    - Rare Words\n",
    "    - URLs\n",
    "    - HTML tags    \n",
    "    - Emojis (or convert them to words)\n",
    "    - Emoticons (or convert them to words)\n",
    "- Conversion: \n",
    "    - Numbers to Words\n",
    "    - Emojis to Words\n",
    "    - Emoticons to Words\n",
    "- Stemming\n",
    "- Lemmatization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a71b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ali/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=60)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "random.seed(55)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798992ae",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "> The [Customer Support on Twitter](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter) dataset is a large, modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models, and for study of modern customer support practices and impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5a9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading just the 'text' column and assuring its data type \n",
    "DATA_ADDRESS = \"../data/archive/twcs/twcs.csv\"\n",
    "df = pd.DataFrame(\n",
    "    data=pd.read_csv(DATA_ADDRESS, nrows=1000)['text'].astype(str),\n",
    "    columns=['text']\n",
    "    )\n",
    "\n",
    "# Keep rows with lenght size bigger than threshold\n",
    "threshold = 200\n",
    "df['length'] = df.text.apply(len)\n",
    "df = df[df.length > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6505176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@marksandspencer aren’t require charge 4 a bag: paper bags\n",
      "shops in airports, or on board trains, aeroplanes or ships\n",
      "bags which only contain certain items, such as unwrapped\n",
      "food, raw meat and fish where there is a food safety risk,\n",
      "prescription medicines, uncovered blades, seeds, bulbs &amp;\n",
      "🌷s\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(df))\n",
    "print(wrapper.fill(df.iloc[idx]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e3834",
   "metadata": {},
   "source": [
    "### Lower Casing\n",
    "\n",
    "For text featurization (e.g. tf-idf) and avoid duplications to get correct counts and/or save tokens. Not recommended for Part of Speech tagging or Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1a8b5fe-7947-4e59-917f-f59fbbbdc2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@marksandspencer aren’t require charge 4 a bag: paper bags\n",
      "shops in airports, or on board trains, aeroplanes or ships\n",
      "bags which only contain certain items, such as unwrapped\n",
      "food, raw meat and fish where there is a food safety risk,\n",
      "prescription medicines, uncovered blades, seeds, bulbs &amp;\n",
      "🌷s\n"
     ]
    }
   ],
   "source": [
    "df.text = df.text.str.lower()\n",
    "print(wrapper.fill(df.iloc[idx]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f380f6",
   "metadata": {},
   "source": [
    "### Remove Punctuations\n",
    "\n",
    "Remove punctuations if they don't carry important information for your use case. \n",
    "For instance:\n",
    "\n",
    "```python\n",
    "> print(string.punctuation)\n",
    "> !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e92fe6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNC_TO_REMOVE = string.punctuation\n",
    "PUNCT_TO_REMOVE_TABLE = str.maketrans('', '', PUNC_TO_REMOVE)\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    return text.translate(PUNCT_TO_REMOVE_TABLE)\n",
    "\n",
    "df.text = df.text.apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13cde198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marksandspencer aren’t require charge 4 bag paper bags shops\n",
      "airports board trains aeroplanes ships bags contain certain\n",
      "items unwrapped food raw meat fish food safety risk\n",
      "prescription medicines uncovered blades seeds bulbs amp 🌷s\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.fill(df.iloc[idx]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150427ff",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f231c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "\n",
    "df.text = df.text.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd9e47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marksandspencer aren’t require charge 4 bag paper bags shops\n",
      "airports board trains aeroplanes ships bags contain certain\n",
      "items unwrapped food raw meat fish food safety risk\n",
      "prescription medicines uncovered blades seeds bulbs amp 🌷s\n"
     ]
    }
   ],
   "source": [
    "print(wrapper.fill(df.iloc[idx]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919041d1",
   "metadata": {},
   "source": [
    "### Frequent Words\n",
    "\n",
    "In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0a18e849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bags', 'food', 'marksandspencer', 'aren’t']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_top_k_frequent_words(text: str, top_k: int, frequent: bool) -> List[str]:\n",
    "    l_text = text.split()\n",
    "    hashCount = defaultdict()\n",
    "    container = [[] for i in range(len(l_text) + 1)]\n",
    "\n",
    "    for word in l_text:\n",
    "        hashCount[word] = 1 + hashCount.get(word, 0)\n",
    "    \n",
    "    for w, c in hashCount.items():\n",
    "        container[c].append(w)\n",
    "\n",
    "    result = []\n",
    "    start, stop, step = -1, 0, -1\n",
    "    if not frequent:\n",
    "        start, stop, step = 1, len(l_text) + 1, 1\n",
    "    for ws in container[start:stop:step]:\n",
    "        for w in ws:\n",
    "            result.append(w)\n",
    "            if len(result) > top_k - 1:\n",
    "                return result\n",
    "                \n",
    "find_top_k_frequent_words(df.iloc[idx]['text'], 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "24d42b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bags', 'food', 'marksandspencer', 'aren’t']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "for word in df.iloc[idx]['text'].split():\n",
    "    cnt[word] += 1\n",
    "\n",
    "[item[0] for item in cnt.most_common(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613185e9",
   "metadata": {},
   "source": [
    "### Rare Words\n",
    "\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "85eafb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bags', 2), ('food', 2), ('marksandspencer', 1), ('aren’t', 1)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7777ade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marksandspencer', 'aren’t', 'require', 'charge']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_top_k_frequent_words(df.iloc[idx]['text'], 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "188121f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RARE_WORDS = [item[0] for item in cnt.most_common()[:-4-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd835bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_words(text: str) -> str:\n",
    "    return \" \".join([word for word in text.split() if word not in RARE_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a242559",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "In Natural Language Processing (NLP), stemming is the process of reducing a word to its base or root form. The goal of stemming is to reduce inflected or derived words to their word stem, base, or root form, which may not necessarily be a valid word itself. This helps in standardizing words and improving the performance of text analysis tasks such as search engines and text classification.\n",
    "\n",
    "For example, the words \"running,\" \"runner,\" and \"ran\" might all be reduced to the root form \"run.\" Similarly, \"studies\" and \"studying\" might be reduced to \"studi.\"\n",
    "\n",
    "Common stemming algorithms include:\n",
    "\n",
    "- Porter Stemmer: One of the most popular stemming algorithms, known for its simplicity and effectiveness.\n",
    "- Snowball Stemmer: An improved version of the Porter Stemmer, also known as the Porter2 stemmer.\n",
    "- Lancaster Stemmer: A more aggressive stemming algorithm compared to Porter and Snowball.\n",
    "\n",
    "Stemming can help in:\n",
    "\n",
    "- Search Engines: By stemming words, search engines can match different forms of a word to provide more comprehensive search results.\n",
    "- Text Classification: Reducing words to their root form can help in grouping similar terms together, making it easier to classify text documents.\n",
    "- Information Retrieval: Improves the recall rate by matching terms with similar roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b6b9e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "poster_stemmer = PorterStemmer()\n",
    "poster2_stemmer = Porter2Stemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "def stem_words(text: str) -> str:\n",
    "    l_text = text.split()\n",
    "    poster_stemmed = \" \".join([poster_stemmer.stem(word) for word in l_text])\n",
    "    poster2_stemmed = \" \".join([poster2_stemmer.stem(word) for word in l_text])\n",
    "    lancaster_stemmed = \" \".join([lancaster_stemmer.stem(word) for word in l_text])\n",
    "    return poster_stemmed, poster2_stemmed, lancaster_stemmed\n",
    "            \n",
    "df[[\"Poster\", \"Poster2\", \"Lancaster\"]] = df.text.apply(lambda text: pd.Series(stem_words(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ad76f",
   "metadata": {},
   "source": [
    "Also this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0bb42ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa9216",
   "metadata": {},
   "source": [
    "However, stemming has its limitations. It can sometimes lead to overstemming, where different words with different meanings are reduced to the same root, or understemming, where words that should be reduced to the same root are not. In such cases, more sophisticated techniques like lemmatization might be preferred, as lemmatization considers the context and grammar to reduce words to their meaningful base form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44f87f",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is a process in Natural Language Processing (NLP) that reduces words to their base or root form, known as a lemma. Unlike stemming, which simply truncates words to remove prefixes or suffixes, lemmatization considers the context and morphological analysis of the words to convert them into meaningful base forms.\n",
    "\n",
    "Key Points of Lemmatization\n",
    "- Context-Aware: Lemmatization uses vocabulary and morphological analysis to correctly transform words. For example, the word \"better\" is lemmatized to \"good,\" and \"am,\" \"are,\" \"is\" are all lemmatized to \"be.\"\n",
    "\n",
    "- Part of Speech (POS) Tagging: Lemmatization often involves part-of-speech tagging, which helps in identifying the correct form of the word. For instance, the word \"running\" can be lemmatized to \"run\" (verb) or \"running\" (noun) based on its usage.\n",
    "\n",
    "- Meaning Preservation: It ensures that the transformed word (lemma) is a valid word that exists in the language, preserving its meaning and grammatical correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de848db",
   "metadata": {},
   "source": [
    "Lemmatization can be implemented using various NLP libraries such as NLTK or spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442da35",
   "metadata": {},
   "source": [
    "#### Lemmatize by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "69b816a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ali/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV}\n",
    "\n",
    "def lemmatize_word_by_nltk(text: str) -> str:\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(\n",
    "        word, wordnet_map.get(pos, wordnet.NOUN)\n",
    "        ) for word, pos in pos_tagged_text]\n",
    "        )\n",
    "\n",
    "df[\"lemmatized_nltk\"] = df.text.apply(lambda text: lemmatize_word_by_nltk(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df012e77",
   "metadata": {},
   "source": [
    "#### Lemmatize by SpaCy\n",
    "\n",
    "You need to install the spacy model, `en_core_web_sm`, before loading it \n",
    "```bash\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36930c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_word_by_spacy(text: str) -> str:\n",
    "    return \" \".join([token.lemma_ for token in lemmatize_model(text)])\n",
    "\n",
    "df[\"lemmatized_spacy\"] = df.text.apply(lambda text: lemmatize_word_by_spacy(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c80c11",
   "metadata": {},
   "source": [
    "### Processign Emoji's and Emoticons\n",
    "\n",
    "Processing emojis and emoticons in NLP is important because these elements carry significant emotional and contextual information. You might want to normalize emojis and emoticons to their textual representations to make them easier to handle. This involves converting them into descriptive text. Two main libraries that are handy are: [`emoji`](https://pypi.org/project/emoji/) and [`emot`](https://github.com/NeelShah18/emot?tab=readme-ov-file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c2ad3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love pizza :pizza:! It's amazing :smiling_face_with_heart-eyes:.\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Example usage\n",
    "text = \"I love pizza 🍕! It's amazing 😍.\"\n",
    "normalized_text = convert_emojis_to_text(text)\n",
    "print(normalized_text)\n",
    "# Output: \"I love pizza :pizza:! It's amazing :heart_eyes:.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "deae0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy Happy_face_smiley but sometimes sad Frown_sad_andry_or_pouting\n"
     ]
    }
   ],
   "source": [
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "\n",
    "def convert_emoticons_to_text(text):\n",
    "    for e in EMOTICONS_EMO:\n",
    "        text = text.replace(e, \"_\".join(EMOTICONS_EMO[e].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"I am happy :-) but sometimes sad :(\"\n",
    "normalized_text = convert_emoticons_to_text(text)\n",
    "print(normalized_text)\n",
    "# Output: \"I am happy happy_face but sometimes sad sad_face\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d451d",
   "metadata": {},
   "source": [
    "#### Remove Emoji's and Emoticons\n",
    "\n",
    "For some applications, it not harmful to remove them if there is no lost of data. However, the best way is converting them to textual data and then apply deduplication methods to avoid redundancies. For instance, removing similar adjacent words/phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")\n",
    "\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS_EMO) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12808a92",
   "metadata": {},
   "source": [
    "### Process URLs\n",
    "\n",
    "For most NLP tasks, removing URLs is a good practice to reduce noise and improve model performance. However, always consider the specific requirements of your application and the nature of your data when deciding whether to remove URLs.\n",
    "\n",
    "#### When to Remove URLs\n",
    "Here are some reasons and cases why you might want to remove URLs from your text data. \n",
    "\n",
    "- Irrelevant Information: URLs often don't contribute meaningful information for text analysis or understanding. They can introduce noise into the dataset.\n",
    "\n",
    "- Consistency: Removing URLs can help standardize the text data, making it easier to process and analyze. URLs can vary greatly in format and content, which can disrupt the consistency of the dataset.\n",
    "\n",
    "- Tokenization: URLs can complicate tokenization, especially if they contain special characters or are very long. Removing them simplifies the tokenization process.\n",
    "\n",
    "- Model Performance: For models focused on semantic analysis, sentiment analysis, topic modeling, or other tasks, URLs usually don't add value and can even degrade model performance by introducing irrelevant features.\n",
    "\n",
    "- Privacy and Security: URLs can contain sensitive information. Removing them can help in protecting privacy and ensuring that no sensitive data is inadvertently included in the analysis.\n",
    "\n",
    "You can use regular expressions (regex) to remove URLs from text data. Here's an example in Python using the re library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2dcc0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba5705",
   "metadata": {},
   "source": [
    "#### When Not to Remove URLs\n",
    "\n",
    "There are scenarios where URLs might contain valuable information:\n",
    "\n",
    "- URL Analysis: If your task involves analyzing the content or structure of URLs themselves, such as in link analysis, web scraping, or cybersecurity tasks.\n",
    "\n",
    "- Information Retrieval: If URLs are being used as identifiers or keys to fetch additional data from the web.\n",
    "\n",
    "- Named Entity Recognition (NER): If you're building models that need to recognize and classify URLs as specific entities within the text.\n",
    "\n",
    "In such cases, rather than removing URLs, you might choose to preprocess them differently, such as by replacing them with a placeholder token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fef471",
   "metadata": {},
   "source": [
    "## Processing HTML Tags\n",
    "\n",
    "For most NLP tasks, removing HTML tags is a beneficial preprocessing step that helps in reducing noise, improving text consistency, and enhancing model performance. However, consider the specific needs of your application and data when deciding whether to remove HTML tags. Here are some methods to remove HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b463db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text with HTML tags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Example usage\n",
    "html_text = \"<p>This is a <b>sample</b> text with <a href='https://example.com'>HTML</a> tags.</p>\"\n",
    "cleaned_text = remove_html_tags(html_text)\n",
    "print(cleaned_text)\n",
    "# Output: \"This is a sample text with HTML tags.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0bb65b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text with HTML tags.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Example usage\n",
    "html_text = \"<p>This is a <b>sample</b> text with <a href='https://example.com'>HTML</a> tags.</p>\"\n",
    "cleaned_text = remove_html_tags(html_text)\n",
    "print(cleaned_text)\n",
    "# Output: \"This is a sample text with HTML tags.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda563a",
   "metadata": {},
   "source": [
    "#### When Not to Remove HTML Tags\n",
    "There are some scenarios where you might want to preserve HTML tags:\n",
    "\n",
    "- HTML Analysis: If your task involves analyzing the structure of HTML documents, such as web scraping or studying the use of specific HTML tags.\n",
    "\n",
    "- Content Extraction: If you need to extract specific elements from the HTML, such as links, images, or metadata.\n",
    "\n",
    "- Rich Text Processing: If the formatting provided by HTML tags is necessary for understanding the text, such as in rich text processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ee7c6",
   "metadata": {},
   "source": [
    "## Process Chat Words Conversion\n",
    "\n",
    "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\n",
    "\n",
    "Got a good list of chat slang words from this repo. We can use this for our conversion [here](https://github.com/rishabhverma17/sms_slang_translator/tree/master). We can add more words to this list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f8388",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edda5ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
